{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10276551,"sourceType":"datasetVersion","datasetId":6358764}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install albumentations segmentation_models_pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T06:21:53.936697Z","iopub.execute_input":"2025-03-04T06:21:53.936936Z","iopub.status.idle":"2025-03-04T06:22:03.530494Z","shell.execute_reply.started":"2025-03-04T06:21:53.936911Z","shell.execute_reply":"2025-03-04T06:22:03.529497Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.15)\nCollecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.24.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\nRequirement already satisfied: albucore>=0.0.15 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.16)\nRequirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nCollecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.24.7)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (10.4.0)\nCollecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.16.0)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.4.1+cu121)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.19.1+cu121)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\nCollecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\nRequirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\nRequirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.35.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.8.30)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=2054d58f71adc7fef24b3cb2f5a266a8e714259ddd4e8e9c6300dfbfd2ec2a0e\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=b489c85f1f0da5468d2f339371f84a06328663e6169983286eface33baaf0564\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation_models_pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport ssl\n\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Set device and handle multiple GPUs\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# ================= Dataset Class =================\nclass RoadDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = os.listdir(img_dir)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.images[idx])\n        mask_path = os.path.join(self.mask_dir, self.images[idx])\n        \n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n        \n        mask = (mask > 0).astype(np.float32)\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n\n        return image, mask.unsqueeze(0)\n\n# ================= Transformations =================\ndef get_transforms(img_size=512):\n    train_transform = A.Compose([\n        A.Resize(img_size, img_size),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ])\n    \n    val_transform = A.Compose([\n        A.Resize(img_size, img_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ])\n    return train_transform, val_transform\n\n# ================= Loss Functions =================\ndef combined_loss(pred, target):\n    focal_loss_fn = smp.losses.FocalLoss(mode=\"binary\", alpha=0.8, gamma=2.0)\n    dice_loss_fn = smp.losses.DiceLoss(mode=\"binary\", from_logits=True)\n    return 0.5 * focal_loss_fn(pred, target) + 0.5 * dice_loss_fn(pred, target)\n\ndef calculate_iou(pred, target):\n    pred = torch.sigmoid(pred) > 0.5\n    pred = pred.bool()\n    target = target.bool()\n    intersection = (pred & target).sum(dim=(2, 3))\n    union = (pred | target).sum(dim=(2, 3))\n    iou = (intersection + 1e-4) / (union + 1e-4)\n    return iou.mean().item()\n\n# ================= Model Initialization =================\ndef get_model(encoder_name=\"resnext101_32x8d\", num_classes=1):\n    model = smp.DeepLabV3Plus(\n        encoder_name=encoder_name,\n        encoder_weights=\"ssl\",\n        in_channels=3,\n        classes=num_classes\n    )\n    return nn.DataParallel(model).to(device)\n\n# ================= Training Function =================\ndef train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader, leave=True)\n    model.train()\n    total_loss = 0\n    total_iou = 0\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device)\n        targets = targets.to(device)\n\n        with torch.cuda.amp.autocast():\n            outputs = model(data)\n            loss = loss_fn(outputs, targets)\n            iou = calculate_iou(outputs, targets)\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        total_iou += iou\n        loop.set_postfix(loss=loss.item(), IoU=iou)\n    \n    return total_loss / len(loader), total_iou / len(loader)\n\n# ================= Validation Function =================\ndef val_fn(loader, model, loss_fn):\n    model.eval()\n    total_loss = 0\n    total_iou = 0\n\n    with torch.no_grad():\n        for data, targets in loader:\n            data = data.to(device)\n            targets = targets.to(device)\n            outputs = model(data)\n            loss = loss_fn(outputs, targets)\n            iou = calculate_iou(outputs, targets)\n            total_loss += loss.item()\n            total_iou += iou\n\n    return total_loss / len(loader), total_iou / len(loader)\n\n# ================= Save Model Function =================\ndef save_model(model, encoder_name, epoch, train_iou, val_iou, best_model_path=None):\n    # Delete previous best model if it exists\n    if best_model_path is not None and os.path.exists(best_model_path):\n        os.remove(best_model_path)\n    \n    # Save new best model\n    checkpoint = {\n        'state_dict': model.state_dict(),\n        'epoch': epoch,\n        'train_iou': train_iou,\n        'val_iou': val_iou\n    }\n    filename = f\"DeepLabV3Plus_{encoder_name}_best_model_val_iou{val_iou:.4f}.pth\"\n    torch.save(checkpoint, filename)\n    return filename\n\n# ================= Main Function =================\ndef main(train_img_dir, train_mask_dir, val_img_dir, val_mask_dir):\n    LEARNING_RATE = 1e-4\n    BATCH_SIZE = 16\n    NUM_EPOCHS = 90\n    IMG_SIZE = 512\n    ENCODER_NAME = \"resnext101_32x8d\"\n\n    train_transform, val_transform = get_transforms(img_size=IMG_SIZE)\n    train_dataset = RoadDataset(train_img_dir, train_mask_dir, transform=train_transform)\n    val_dataset = RoadDataset(val_img_dir, val_mask_dir, transform=val_transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)\n\n    model = get_model(encoder_name=ENCODER_NAME)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    scaler = torch.cuda.amp.GradScaler()\n\n    # Initialize best scores and model path\n    best_val_iou = 0.0\n    best_model_path = None\n\n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n        train_loss, train_iou = train_fn(train_loader, model, optimizer, combined_loss, scaler)\n        val_loss, val_iou = val_fn(val_loader, model, combined_loss)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}\")\n\n        # Save model if validation IoU improves\n        if val_iou > best_val_iou:\n            best_val_iou = val_iou\n            best_model_path = save_model(model, ENCODER_NAME, epoch + 1, train_iou, val_iou, best_model_path)\n            print(f\"New best model saved! Validation IoU: {val_iou:.4f}\")\n\n    print(\"\\nTraining completed!\")\n    print(f\"Best validation IoU: {best_val_iou:.4f}\")\n    print(f\"Best model saved as: {best_model_path}\")\n\nif __name__ == '__main__':\n    # Kaggle-specific paths (modify as needed)\n    TRAIN_IMG_DIR = '/kaggle/input/r-shanghai/cmp_data(shanghai)/train/img'\n    TRAIN_MASK_DIR = '/kaggle/input/r-shanghai/cmp_data(shanghai)/train/mask'\n    VAL_IMG_DIR = '/kaggle/input/r-shanghai/cmp_data(shanghai)/val/img'\n    VAL_MASK_DIR = '/kaggle/input/r-shanghai/cmp_data(shanghai)/val/mask'\n    \n    main(TRAIN_IMG_DIR, TRAIN_MASK_DIR, VAL_IMG_DIR, VAL_MASK_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T05:11:08.372177Z","iopub.execute_input":"2025-01-08T05:11:08.372577Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth\" to /root/.cache/torch/hub/checkpoints/semi_supervised_resnext101_32x8-2cfe2f8b.pth\n100%|██████████| 340M/340M [00:05<00:00, 62.9MB/s] \n<ipython-input-2-7bf5ec41c2c9>:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/90\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/103 [00:00<?, ?it/s]<ipython-input-2-7bf5ec41c2c9>:101: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 103/103 [01:17<00:00,  1.32it/s, IoU=0.31, loss=0.337] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3479, Train IoU: 0.2268\nVal Loss: 0.2784, Val IoU: 0.3596\nNew best model saved! Validation IoU: 0.3596\n\nEpoch 2/90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103/103 [01:21<00:00,  1.27it/s, IoU=0.361, loss=0.294]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2697, Train IoU: 0.3673\nVal Loss: 0.2278, Val IoU: 0.4173\nNew best model saved! Validation IoU: 0.4173\n\nEpoch 3/90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103/103 [01:20<00:00,  1.28it/s, IoU=0.493, loss=0.174]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2353, Train IoU: 0.4190\nVal Loss: 0.2117, Val IoU: 0.4385\nNew best model saved! Validation IoU: 0.4385\n\nEpoch 4/90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103/103 [01:20<00:00,  1.28it/s, IoU=0.535, loss=0.114]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2179, Train IoU: 0.4480\nVal Loss: 0.2021, Val IoU: 0.4651\nNew best model saved! Validation IoU: 0.4651\n\nEpoch 5/90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103/103 [01:20<00:00,  1.28it/s, IoU=0.503, loss=0.227]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2009, Train IoU: 0.4787\nVal Loss: 0.1951, Val IoU: 0.4898\nNew best model saved! Validation IoU: 0.4898\n\nEpoch 6/90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103/103 [01:20<00:00,  1.28it/s, IoU=0.458, loss=0.235]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2008, Train IoU: 0.4828\nVal Loss: 0.1838, Val IoU: 0.5015\nNew best model saved! Validation IoU: 0.5015\n\nEpoch 7/90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103/103 [01:20<00:00,  1.28it/s, IoU=0.54, loss=0.104] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1814, Train IoU: 0.5059\nVal Loss: 0.1924, Val IoU: 0.4925\n\nEpoch 8/90\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 99/103 [01:17<00:03,  1.30it/s, IoU=0.499, loss=0.226] ","output_type":"stream"}],"execution_count":null}]}